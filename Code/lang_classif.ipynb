{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"076hHg-sWk3O","outputId":"1d88d81d-1953-4c7c-c8b6-88be7cd749ae","executionInfo":{"status":"ok","timestamp":1672674812600,"user_tz":-60,"elapsed":57448,"user":{"displayName":"Gi Part","userId":"17782020154220916187"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting flair\n","  Downloading flair-0.11.3-py3-none-any.whl (401 kB)\n","\u001b[K     |████████████████████████████████| 401 kB 29.1 MB/s \n","\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from flair) (4.9.2)\n","Collecting sqlitedict>=1.6.0\n","  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from flair) (1.0.2)\n","Collecting bpemb>=0.3.2\n","  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n","Collecting ftfy\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.8/dist-packages (from flair) (3.2.2)\n","Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from flair) (3.6.0)\n","Collecting janome\n","  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n","\u001b[K     |████████████████████████████████| 19.7 MB 1.9 MB/s \n","\u001b[?25hCollecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[K     |████████████████████████████████| 981 kB 49.7 MB/s \n","\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n","  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n","Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.64.1)\n","Collecting sentencepiece==0.1.95\n","  Downloading sentencepiece-0.1.95-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 56.1 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from flair) (2022.6.2)\n","Requirement already satisfied: gdown==4.4.0 in /usr/local/lib/python3.8/dist-packages (from flair) (4.4.0)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.8/dist-packages (from flair) (2.8.2)\n","Collecting segtok>=1.5.7\n","  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n","Collecting pptree\n","  Downloading pptree-3.1.tar.gz (3.0 kB)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from flair) (9.0.0)\n","Collecting wikipedia-api\n","  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n","Collecting deprecated>=1.2.4\n","  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n","Collecting mpld3==0.3\n","  Downloading mpld3-0.3.tar.gz (788 kB)\n","\u001b[K     |████████████████████████████████| 788 kB 60.1 MB/s \n","\u001b[?25hCollecting huggingface-hub\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 68.5 MB/s \n","\u001b[?25hRequirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from flair) (1.13.0+cu116)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from flair) (0.8.10)\n","Collecting hyperopt>=0.2.7\n","  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 55.5 MB/s \n","\u001b[?25hCollecting transformers>=4.0.0\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 60.0 MB/s \n","\u001b[?25hCollecting conllu>=4.0\n","  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (1.15.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (2.23.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (4.6.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown==4.4.0->flair) (3.8.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from bpemb>=0.3.2->flair) (1.21.6)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from deprecated>=1.2.4->flair) (1.14.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->flair) (1.7.3)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->flair) (6.3.0)\n","Collecting py4j\n","  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n","\u001b[K     |████████████████████████████████| 200 kB 64.7 MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (0.16.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (1.5.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.7->flair) (2.8.8)\n","Collecting requests\n","  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.2 MB/s \n","\u001b[?25hCollecting overrides<4.0.0,>=3.0.0\n","  Downloading overrides-3.1.0.tar.gz (11 kB)\n","Collecting importlib-metadata<4.0.0,>=3.7.0\n","  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->bpemb>=0.3.2->flair) (2.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->bpemb>=0.3.2->flair) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair) (1.2.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch!=1.8,>=1.5.0->flair) (4.4.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.0.0->flair) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.0.0->flair) (21.3)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 32.0 MB/s \n","\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->flair) (0.2.5)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n","Building wheels for collected packages: mpld3, overrides, sqlitedict, langdetect, pptree\n","  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=a19459d452200b6fe752acccc8764e0dfdcb818e9a4612ccfd74c8fed655c430\n","  Stored in directory: /root/.cache/pip/wheels/3d/9f/9d/d806a20bd97bc7076d724fa3e69fa5be61836ba16b2ffa6126\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=f46357dda99339aaf93f1f3afce475a1866c7f25ee8c996442cd7c537c15413c\n","  Stored in directory: /root/.cache/pip/wheels/6a/4f/72/28857f75625b263e2e3f5ab2fc4416c0a85960ac6485007eaa\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16869 sha256=f9f578fdf8783ec376cddebaed86d91b3d30cd19f1947bb42f6d05829e726788\n","  Stored in directory: /root/.cache/pip/wheels/04/c6/16/46e174009277f9bccdaa7215a243939d2f70180804b249bf3a\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=321209d1cebeeda5a8b9a9b8f152e1e7401970f2296de6e24adcfc21a4401779\n","  Stored in directory: /root/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n","  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=29d4c24b25ffa852495789e1d8bcb424e32e50d606dbda4f9b7dab52669bac67\n","  Stored in directory: /root/.cache/pip/wheels/e1/8b/30/5b20240d3d13a9dfafb6a6dd49d1b541c86d39812cb3690edf\n","Successfully built mpld3 overrides sqlitedict langdetect pptree\n","Installing collected packages: requests, tokenizers, sentencepiece, py4j, overrides, importlib-metadata, huggingface-hub, wikipedia-api, transformers, sqlitedict, segtok, pptree, mpld3, langdetect, konoha, janome, hyperopt, ftfy, deprecated, conllu, bpemb, flair\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 5.1.0\n","    Uninstalling importlib-metadata-5.1.0:\n","      Successfully uninstalled importlib-metadata-5.1.0\n","  Attempting uninstall: hyperopt\n","    Found existing installation: hyperopt 0.1.2\n","    Uninstalling hyperopt-0.1.2:\n","      Successfully uninstalled hyperopt-0.1.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","markdown 3.4.1 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n","gym 0.25.2 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\u001b[0m\n","Successfully installed bpemb-0.3.4 conllu-4.5.2 deprecated-1.2.13 flair-0.11.3 ftfy-6.1.1 huggingface-hub-0.11.1 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 pptree-3.1 py4j-0.10.9.7 requests-2.28.1 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.1.0 tokenizers-0.13.2 transformers-4.25.1 wikipedia-api-0.5.8\n"]}],"source":["!pip install torch\n","!pip install flair"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","import torch\n","from flair.datasets import CSVClassificationCorpus\n","from flair.data import Corpus\n","from flair.embeddings import TransformerDocumentEmbeddings\n","from flair.models import TextClassifier\n","from flair.trainers import ModelTrainer"],"metadata":{"id":"gKBog1pgjP6N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# mount the google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NbxCpsN1MXDa","outputId":"7692150d-39c2-472c-8d8b-e0a0e430f784","executionInfo":{"status":"ok","timestamp":1672674855008,"user_tz":-60,"elapsed":34511,"user":{"displayName":"Gi Part","userId":"17782020154220916187"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["# Import the training and test sets\n","df_train = pd.read_json(\"./classifier_data_train.json\", lines=True)[[\"text\",\"lang\"]]\n","df_test = pd.read_json(\"./classifier_data_eval.json\", lines=True)[[\"text\",\"lang\"]]\n","print(f\"Original overall dataset length: {len(df_train)+len(df_test)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U4zVMFaPjP9H","outputId":"6faeb395-6021-48d7-a003-e42a950641b8","executionInfo":{"status":"ok","timestamp":1672490945347,"user_tz":-60,"elapsed":1271,"user":{"displayName":"Tutor SNLP","userId":"05874048045971408079"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original overall dataset length: 43958\n"]}]},{"cell_type":"code","source":["# Check for duplicates in whole dataset and remove if existent (34 dupls in train_set, 10 in test_set)\n","df_total = pd.concat([df_train, df_test]).reset_index(drop=True)\n","dupls = df_total[df_total.duplicated(keep=\"first\")]\n","print(f\"Total number of duplicates: {len(dupls)}\")\n","df_total = df_total.drop_duplicates()\n","print(f\"Overall dataset length after duplicate removal: {len(df_total)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qc36f7ay0H4O","outputId":"2e88a896-dc6d-4c3f-cee7-44d3e1e2b464","executionInfo":{"status":"ok","timestamp":1672490947524,"user_tz":-60,"elapsed":830,"user":{"displayName":"Tutor SNLP","userId":"05874048045971408079"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of duplicates: 44\n","Overall dataset length after duplicate removal: 43914\n"]}]},{"cell_type":"code","source":["# Split dataset in train, dev, test and shuffle\n","df_test = df_total.iloc[-4871:].reset_index(drop=True)\n","df_test = df_test.sample(frac=1, random_state=1000).reset_index(drop=True)\n","#print(len(df_test))\n","\n","df_total = df_total.drop(index=df_total.index[-4871:], axis=0)\n","df_train = df_total.sample(frac=1, random_state=1000).reset_index(drop=True)\n","#print(len(df_train))\n","\n","df_dev = df_train.iloc[-4871:].reset_index(drop=True)\n","df_dev = df_dev.sample(frac=1, random_state=1000).reset_index(drop=True)\n","#print(len(df_dev))\n","\n","df_train = df_train.drop(index=df_train.index[-4871:], axis=0)\n","print(f\"Train: {len(df_train)}, Dev: {len(df_dev)}, Test: {len(df_test)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdQOHy28fl4s","outputId":"061e0bfe-9973-466f-8dfe-0ac495be4ba1","executionInfo":{"status":"ok","timestamp":1672490948348,"user_tz":-60,"elapsed":5,"user":{"displayName":"Tutor SNLP","userId":"05874048045971408079"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train: 34172, Dev: 4871, Test: 4871\n"]}]},{"cell_type":"code","source":["# Save in csv format\n","df_train.to_csv(\"train.csv\", index=False)\n","df_dev.to_csv(\"dev.csv\", index=False)\n","df_test.to_csv(\"test.csv\", index=False)\n"],"metadata":{"id":"KHN6L3QVbPFO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# What label do we want to predict?\n","label_type = 'language_identification'\n","\n","# this is the folder in which train, test and dev files reside\n","data_folder = '/content/data'\n","\n","# column format indicating which columns hold the text and label(s)\n","column_name_map = {0: \"text\", 1: \"label_lang\"}\n","\n","# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n","corpus: Corpus = CSVClassificationCorpus(data_folder=data_folder,\n","                                         column_name_map=column_name_map,\n","                                         skip_header=True,\n","                                         delimiter=',',\n","                                         label_type=label_type)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2VyrcW_xYjWu","outputId":"d15314d6-85f5-4a21-a278-3c674b451f0b","executionInfo":{"status":"ok","timestamp":1672674953196,"user_tz":-60,"elapsed":650,"user":{"displayName":"Gi Part","userId":"17782020154220916187"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-01-02 15:55:52,458 Reading data from /content/data\n","2023-01-02 15:55:52,460 Train: /content/data/train.csv\n","2023-01-02 15:55:52,463 Dev: /content/data/dev.csv\n","2023-01-02 15:55:52,465 Test: /content/data/test.csv\n"]}]},{"cell_type":"code","source":["label_dict = corpus.make_label_dictionary(label_type=label_type)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PTbFhEvyt--","outputId":"7b2ecd58-9ccd-4bee-9cf0-0bc3e3536781","executionInfo":{"status":"ok","timestamp":1672675003563,"user_tz":-60,"elapsed":42194,"user":{"displayName":"Gi Part","userId":"17782020154220916187"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-01-02 15:56:01,284 Computing label dictionary. Progress:\n"]},{"output_type":"stream","name":"stderr","text":["34172it [00:41, 830.15it/s] "]},{"output_type":"stream","name":"stdout","text":["2023-01-02 15:56:42,496 Dictionary created for label 'language_identification' with 9 values: en (seen 17630 times), de (seen 10128 times), da (seen 5873 times), fr (seen 243 times), it (seen 203 times), unknown (seen 91 times), hu (seen 3 times), sv (seen 1 times)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# initialize transformer document embeddings (many models are available)\n","document_embeddings = TransformerDocumentEmbeddings('xlm-roberta-base', fine_tune=True)\n","\n","# create the text classifier\n","classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, label_type=label_type)\n","\n","\n","# initialize trainer\n","trainer = ModelTrainer(classifier, corpus)\n","\n","# run training with fine-tuning\n","trainer.fine_tune('/content/gdrive/MyDrive/models/language_identification_letters',\n","                  learning_rate=5e-5,\n","                  mini_batch_size=8,\n","                  max_epochs=10,\n","                  optimizer=torch.optim.AdamW,\n","                  embeddings_storage_mode='none',\n","                  checkpoint=True,\n","                  write_weights=True,\n","                  use_final_model_for_eval=False\n","                  )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9a6iaRQtMpXU","outputId":"b3f2433b-cc97-4e2e-fe06-08cac47267de","executionInfo":{"status":"ok","timestamp":1672684164050,"user_tz":-60,"elapsed":8983863,"user":{"displayName":"Gi Part","userId":"17782020154220916187"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-01-02 15:59:49,017 ----------------------------------------------------------------------------------------------------\n","2023-01-02 15:59:49,024 Model: \"TextClassifier(\n","  (decoder): Linear(in_features=768, out_features=9, bias=True)\n","  (dropout): Dropout(p=0.0, inplace=False)\n","  (locked_dropout): LockedDropout(p=0.0)\n","  (word_dropout): WordDropout(p=0.0)\n","  (loss_function): CrossEntropyLoss()\n","  (document_embeddings): TransformerDocumentEmbeddings(\n","    (model): XLMRobertaModel(\n","      (embeddings): XLMRobertaEmbeddings(\n","        (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","        (position_embeddings): Embedding(514, 768, padding_idx=1)\n","        (token_type_embeddings): Embedding(1, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): XLMRobertaEncoder(\n","        (layer): ModuleList(\n","          (0): XLMRobertaLayer(\n","            (attention): XLMRobertaAttention(\n","              (self): XLMRobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): XLMRobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): XLMRobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): XLMRobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): XLMRobertaLayer(\n","            (attention): XLMRobertaAttention(\n","              (self): XLMRobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): XLMRobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): XLMRobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): XLMRobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): XLMRobertaLayer(\n","            (attention): XLMRobertaAttention(\n","              (self): XLMRobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): XLMRobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): XLMRobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): XLMRobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): XLMRobertaLayer(\n","            (attention): XLMRobertaAttention(\n","              (self): XLMRobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): XLMRobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): XLMRobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): XLMRobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): XLMRobertaLayer(\n","            (attention): XLMRobertaAttention(\n","              (self): XLMRobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): XLMRobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): XLMRobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): XLMRobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): XLMRobertaLayer(\n","            (attention): XLMRobertaAttention(\n","              (self): XLMRobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): XLMRobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): XLMRobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): XLMRobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): XLMRobertaLayer(\n","            (attention): XLMRobertaAttention(\n","              (self): XLMRobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): XLMRobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): XLMRobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): XLMRobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): XLMRobertaLayer(\n","            (attention): XLMRobertaAttention(\n","              (self): XLMRobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): XLMRobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): XLMRobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): XLMRobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): XLMRobertaLayer(\n","            (attention): XLMRobertaAttention(\n","              (self): XLMRobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): XLMRobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): XLMRobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): XLMRobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): XLMRobertaLayer(\n","            (attention): XLMRobertaAttention(\n","              (self): XLMRobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): XLMRobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): XLMRobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): XLMRobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): XLMRobertaLayer(\n","            (attention): XLMRobertaAttention(\n","              (self): XLMRobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): XLMRobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): XLMRobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): XLMRobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): XLMRobertaLayer(\n","            (attention): XLMRobertaAttention(\n","              (self): XLMRobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): XLMRobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): XLMRobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): XLMRobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): XLMRobertaPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","  )\n","  (weights): None\n","  (weight_tensor) None\n",")\"\n","2023-01-02 15:59:49,031 ----------------------------------------------------------------------------------------------------\n","2023-01-02 15:59:49,033 Corpus: \"Corpus: 34172 train + 4871 dev + 4871 test sentences\"\n","2023-01-02 15:59:49,038 ----------------------------------------------------------------------------------------------------\n","2023-01-02 15:59:49,040 Parameters:\n","2023-01-02 15:59:49,042  - learning_rate: \"0.000050\"\n","2023-01-02 15:59:49,044  - mini_batch_size: \"8\"\n","2023-01-02 15:59:49,047  - patience: \"3\"\n","2023-01-02 15:59:49,049  - anneal_factor: \"0.5\"\n","2023-01-02 15:59:49,051  - max_epochs: \"10\"\n","2023-01-02 15:59:49,053  - shuffle: \"True\"\n","2023-01-02 15:59:49,054  - train_with_dev: \"False\"\n","2023-01-02 15:59:49,056  - batch_growth_annealing: \"False\"\n","2023-01-02 15:59:49,058 ----------------------------------------------------------------------------------------------------\n","2023-01-02 15:59:49,060 Model training base path: \"/content/gdrive/MyDrive/models/language_identification_letters\"\n","2023-01-02 15:59:49,062 ----------------------------------------------------------------------------------------------------\n","2023-01-02 15:59:49,064 Device: cuda:0\n","2023-01-02 15:59:49,072 ----------------------------------------------------------------------------------------------------\n","2023-01-02 15:59:49,076 Embeddings storage mode: none\n","2023-01-02 15:59:49,077 ----------------------------------------------------------------------------------------------------\n","2023-01-02 16:02:02,621 epoch 1 - iter 427/4272 - loss 0.12342283 - samples/sec: 26.34 - lr: 0.000005\n","2023-01-02 16:04:19,484 epoch 1 - iter 854/4272 - loss 0.06444741 - samples/sec: 26.58 - lr: 0.000010\n","2023-01-02 16:06:35,562 epoch 1 - iter 1281/4272 - loss 0.04409511 - samples/sec: 26.74 - lr: 0.000015\n","2023-01-02 16:08:51,870 epoch 1 - iter 1708/4272 - loss 0.03400212 - samples/sec: 26.71 - lr: 0.000020\n","2023-01-02 16:11:07,950 epoch 1 - iter 2135/4272 - loss 0.02790533 - samples/sec: 26.69 - lr: 0.000025\n","2023-01-02 16:13:23,651 epoch 1 - iter 2562/4272 - loss 0.02384709 - samples/sec: 26.84 - lr: 0.000030\n","2023-01-02 16:15:38,923 epoch 1 - iter 2989/4272 - loss 0.02089520 - samples/sec: 26.85 - lr: 0.000035\n","2023-01-02 16:17:56,987 epoch 1 - iter 3416/4272 - loss 0.01899056 - samples/sec: 26.35 - lr: 0.000040\n","2023-01-02 16:20:12,585 epoch 1 - iter 3843/4272 - loss 0.01745305 - samples/sec: 26.84 - lr: 0.000045\n","2023-01-02 16:22:28,151 epoch 1 - iter 4270/4272 - loss 0.01621595 - samples/sec: 26.84 - lr: 0.000050\n","2023-01-02 16:22:33,747 ----------------------------------------------------------------------------------------------------\n","2023-01-02 16:22:33,752 EPOCH 1 done: loss 0.0162 - lr 0.000050\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 609/609 [00:41<00:00, 14.68it/s]"]},{"output_type":"stream","name":"stdout","text":["2023-01-02 16:23:15,286 Evaluating as a multi-label problem: False\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["2023-01-02 16:23:15,333 DEV : loss 0.004855041857808828 - f1-score (micro avg)  0.9961\n","2023-01-02 16:23:20,626 BAD EPOCHS (no improvement): 4\n","2023-01-02 16:23:35,684 saving best model\n","2023-01-02 16:23:52,472 ----------------------------------------------------------------------------------------------------\n","2023-01-02 16:26:07,647 epoch 2 - iter 427/4272 - loss 0.00567982 - samples/sec: 26.22 - lr: 0.000049\n","2023-01-02 16:28:24,379 epoch 2 - iter 854/4272 - loss 0.00522102 - samples/sec: 26.62 - lr: 0.000049\n","2023-01-02 16:30:40,445 epoch 2 - iter 1281/4272 - loss 0.00493946 - samples/sec: 26.75 - lr: 0.000048\n","2023-01-02 16:32:57,602 epoch 2 - iter 1708/4272 - loss 0.00506788 - samples/sec: 26.54 - lr: 0.000048\n","2023-01-02 16:35:15,283 epoch 2 - iter 2135/4272 - loss 0.00480833 - samples/sec: 26.42 - lr: 0.000047\n","2023-01-02 16:37:31,332 epoch 2 - iter 2562/4272 - loss 0.00474412 - samples/sec: 26.68 - lr: 0.000047\n","2023-01-02 16:39:47,020 epoch 2 - iter 2989/4272 - loss 0.00471931 - samples/sec: 26.85 - lr: 0.000046\n","2023-01-02 16:42:02,442 epoch 2 - iter 3416/4272 - loss 0.00460535 - samples/sec: 26.86 - lr: 0.000046\n","2023-01-02 16:44:16,702 epoch 2 - iter 3843/4272 - loss 0.00438144 - samples/sec: 27.10 - lr: 0.000045\n","2023-01-02 16:46:33,333 epoch 2 - iter 4270/4272 - loss 0.00423913 - samples/sec: 26.58 - lr: 0.000044\n","2023-01-02 16:46:38,841 ----------------------------------------------------------------------------------------------------\n","2023-01-02 16:46:38,844 EPOCH 2 done: loss 0.0042 - lr 0.000044\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 609/609 [00:41<00:00, 14.62it/s]"]},{"output_type":"stream","name":"stdout","text":["2023-01-02 16:47:20,527 Evaluating as a multi-label problem: False\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["2023-01-02 16:47:20,578 DEV : loss 0.0031003127805888653 - f1-score (micro avg)  0.9961\n","2023-01-02 16:47:25,568 BAD EPOCHS (no improvement): 4\n","2023-01-02 16:47:39,885 ----------------------------------------------------------------------------------------------------\n","2023-01-02 16:49:52,583 epoch 3 - iter 427/4272 - loss 0.00380427 - samples/sec: 26.64 - lr: 0.000044\n","2023-01-02 16:52:09,082 epoch 3 - iter 854/4272 - loss 0.00433140 - samples/sec: 26.68 - lr: 0.000043\n","2023-01-02 16:54:25,539 epoch 3 - iter 1281/4272 - loss 0.00350408 - samples/sec: 26.59 - lr: 0.000043\n","2023-01-02 16:56:41,076 epoch 3 - iter 1708/4272 - loss 0.00406211 - samples/sec: 26.93 - lr: 0.000042\n","2023-01-02 16:58:57,469 epoch 3 - iter 2135/4272 - loss 0.00425862 - samples/sec: 26.67 - lr: 0.000042\n","2023-01-02 17:01:13,948 epoch 3 - iter 2562/4272 - loss 0.00440340 - samples/sec: 26.59 - lr: 0.000041\n","2023-01-02 17:03:29,441 epoch 3 - iter 2989/4272 - loss 0.00468224 - samples/sec: 26.83 - lr: 0.000041\n","2023-01-02 17:05:45,936 epoch 3 - iter 3416/4272 - loss 0.00504390 - samples/sec: 26.65 - lr: 0.000040\n","2023-01-02 17:08:00,645 epoch 3 - iter 3843/4272 - loss 0.00523107 - samples/sec: 26.99 - lr: 0.000039\n","2023-01-02 17:10:15,815 epoch 3 - iter 4270/4272 - loss 0.00548734 - samples/sec: 26.91 - lr: 0.000039\n","2023-01-02 17:10:21,314 ----------------------------------------------------------------------------------------------------\n","2023-01-02 17:10:21,317 EPOCH 3 done: loss 0.0055 - lr 0.000039\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 609/609 [00:42<00:00, 14.34it/s]"]},{"output_type":"stream","name":"stdout","text":["2023-01-02 17:11:03,843 Evaluating as a multi-label problem: False\n","2023-01-02 17:11:03,890 DEV : loss 0.004632778000086546 - f1-score (micro avg)  0.9895\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["2023-01-02 17:11:08,990 BAD EPOCHS (no improvement): 4\n","2023-01-02 17:11:23,609 ----------------------------------------------------------------------------------------------------\n","2023-01-02 17:13:35,676 epoch 4 - iter 427/4272 - loss 0.00485971 - samples/sec: 26.71 - lr: 0.000038\n","2023-01-02 17:15:52,674 epoch 4 - iter 854/4272 - loss 0.00420765 - samples/sec: 26.72 - lr: 0.000038\n","2023-01-02 17:18:09,672 epoch 4 - iter 1281/4272 - loss 0.00449899 - samples/sec: 26.57 - lr: 0.000037\n","2023-01-02 17:20:25,528 epoch 4 - iter 1708/4272 - loss 0.00473229 - samples/sec: 26.74 - lr: 0.000037\n","2023-01-02 17:22:41,011 epoch 4 - iter 2135/4272 - loss 0.00457907 - samples/sec: 26.79 - lr: 0.000036\n","2023-01-02 17:24:57,553 epoch 4 - iter 2562/4272 - loss 0.00486137 - samples/sec: 26.65 - lr: 0.000036\n","2023-01-02 17:27:12,779 epoch 4 - iter 2989/4272 - loss 0.00490120 - samples/sec: 26.86 - lr: 0.000035\n","2023-01-02 17:29:28,191 epoch 4 - iter 3416/4272 - loss 0.00493467 - samples/sec: 26.88 - lr: 0.000034\n","2023-01-02 17:31:44,176 epoch 4 - iter 3843/4272 - loss 0.00466401 - samples/sec: 26.74 - lr: 0.000034\n","2023-01-02 17:33:59,258 epoch 4 - iter 4270/4272 - loss 0.00465619 - samples/sec: 26.88 - lr: 0.000033\n","2023-01-02 17:34:04,596 ----------------------------------------------------------------------------------------------------\n","2023-01-02 17:34:04,599 EPOCH 4 done: loss 0.0047 - lr 0.000033\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 609/609 [00:41<00:00, 14.75it/s]"]},{"output_type":"stream","name":"stdout","text":["2023-01-02 17:34:45,924 Evaluating as a multi-label problem: False\n","2023-01-02 17:34:45,972 DEV : loss 0.005278266966342926 - f1-score (micro avg)  0.9908\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["2023-01-02 17:34:50,921 BAD EPOCHS (no improvement): 4\n","2023-01-02 17:35:06,011 ----------------------------------------------------------------------------------------------------\n","2023-01-02 17:37:19,269 epoch 5 - iter 427/4272 - loss 0.00230700 - samples/sec: 26.50 - lr: 0.000033\n","2023-01-02 17:39:35,131 epoch 5 - iter 854/4272 - loss 0.00487898 - samples/sec: 26.79 - lr: 0.000032\n","2023-01-02 17:41:48,974 epoch 5 - iter 1281/4272 - loss 0.00590718 - samples/sec: 27.21 - lr: 0.000032\n","2023-01-02 17:44:04,035 epoch 5 - iter 1708/4272 - loss 0.00583948 - samples/sec: 26.88 - lr: 0.000031\n","2023-01-02 17:46:20,538 epoch 5 - iter 2135/4272 - loss 0.00555001 - samples/sec: 26.64 - lr: 0.000031\n","2023-01-02 17:48:37,575 epoch 5 - iter 2562/4272 - loss 0.00490773 - samples/sec: 26.55 - lr: 0.000030\n","2023-01-02 17:50:53,571 epoch 5 - iter 2989/4272 - loss 0.00457035 - samples/sec: 26.76 - lr: 0.000029\n","2023-01-02 17:53:09,019 epoch 5 - iter 3416/4272 - loss 0.00488828 - samples/sec: 26.80 - lr: 0.000029\n","2023-01-02 17:55:24,929 epoch 5 - iter 3843/4272 - loss 0.00497162 - samples/sec: 26.78 - lr: 0.000028\n","2023-01-02 17:57:41,711 epoch 5 - iter 4270/4272 - loss 0.00513865 - samples/sec: 26.53 - lr: 0.000028\n","2023-01-02 17:57:47,158 ----------------------------------------------------------------------------------------------------\n","2023-01-02 17:57:47,165 EPOCH 5 done: loss 0.0051 - lr 0.000028\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 609/609 [00:41<00:00, 14.56it/s]"]},{"output_type":"stream","name":"stdout","text":["2023-01-02 17:58:29,037 Evaluating as a multi-label problem: False\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["2023-01-02 17:58:29,079 DEV : loss 0.0043687643483281136 - f1-score (micro avg)  0.991\n","2023-01-02 17:58:34,067 BAD EPOCHS (no improvement): 4\n","2023-01-02 17:58:48,408 ----------------------------------------------------------------------------------------------------\n","2023-01-02 18:01:00,624 epoch 6 - iter 427/4272 - loss 0.00415968 - samples/sec: 26.76 - lr: 0.000027\n","2023-01-02 18:03:16,546 epoch 6 - iter 854/4272 - loss 0.00666168 - samples/sec: 26.81 - lr: 0.000027\n","2023-01-02 18:05:32,732 epoch 6 - iter 1281/4272 - loss 0.00627924 - samples/sec: 26.61 - lr: 0.000026\n","2023-01-02 18:07:47,993 epoch 6 - iter 1708/4272 - loss 0.00747138 - samples/sec: 26.92 - lr: 0.000026\n","2023-01-02 18:10:05,087 epoch 6 - iter 2135/4272 - loss 0.00739465 - samples/sec: 26.48 - lr: 0.000025\n","2023-01-02 18:12:19,370 epoch 6 - iter 2562/4272 - loss 0.00728222 - samples/sec: 27.07 - lr: 0.000024\n","2023-01-02 18:14:36,331 epoch 6 - iter 2989/4272 - loss 0.00720937 - samples/sec: 26.56 - lr: 0.000024\n","2023-01-02 18:16:50,828 epoch 6 - iter 3416/4272 - loss 0.00744812 - samples/sec: 27.01 - lr: 0.000023\n","2023-01-02 18:19:08,052 epoch 6 - iter 3843/4272 - loss 0.00736646 - samples/sec: 26.50 - lr: 0.000023\n","2023-01-02 18:21:24,758 epoch 6 - iter 4270/4272 - loss 0.00733949 - samples/sec: 26.61 - lr: 0.000022\n","2023-01-02 18:21:30,201 ----------------------------------------------------------------------------------------------------\n","2023-01-02 18:21:30,203 EPOCH 6 done: loss 0.0073 - lr 0.000022\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 609/609 [00:41<00:00, 14.73it/s]"]},{"output_type":"stream","name":"stdout","text":["2023-01-02 18:22:11,588 Evaluating as a multi-label problem: False\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["2023-01-02 18:22:11,664 DEV : loss 0.00709431292489171 - f1-score (micro avg)  0.9897\n","2023-01-02 18:22:17,902 BAD EPOCHS (no improvement): 4\n","2023-01-02 18:22:32,828 ----------------------------------------------------------------------------------------------------\n","2023-01-02 18:24:45,587 epoch 7 - iter 427/4272 - loss 0.00646092 - samples/sec: 26.41 - lr: 0.000022\n","2023-01-02 18:27:02,209 epoch 7 - iter 854/4272 - loss 0.00647616 - samples/sec: 26.82 - lr: 0.000021\n","2023-01-02 18:27:53,046 ----------------------------------------------------------------------------------------------------\n","2023-01-02 18:27:53,051 Exiting from training early.\n","2023-01-02 18:27:53,054 Saving model ...\n","2023-01-02 18:28:07,902 Done.\n","2023-01-02 18:28:07,904 ----------------------------------------------------------------------------------------------------\n","2023-01-02 18:28:07,911 loading file /content/gdrive/MyDrive/models/language_identification_letters/best-model.pt\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 609/609 [00:44<00:00, 13.61it/s]"]},{"output_type":"stream","name":"stdout","text":["2023-01-02 18:29:23,522 Evaluating as a multi-label problem: False\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["2023-01-02 18:29:23,621 0.9951\t0.9951\t0.9951\t0.9951\n","2023-01-02 18:29:23,622 \n","Results:\n","- F-score (micro) 0.9951\n","- F-score (macro) 0.8062\n","- Accuracy 0.9951\n","\n","By class:\n","              precision    recall  f1-score   support\n","\n","          en     0.9984    0.9972    0.9978      2517\n","          de     0.9965    0.9993    0.9979      1436\n","          da     0.9941    1.0000    0.9970       836\n","          fr     0.8780    1.0000    0.9351        36\n","          it     0.8571    0.9677    0.9091        31\n","     unknown     0.0000    0.0000    0.0000        15\n","\n","    accuracy                         0.9951      4871\n","   macro avg     0.7874    0.8274    0.8062      4871\n","weighted avg     0.9922    0.9951    0.9936      4871\n","\n","2023-01-02 18:29:23,625 ----------------------------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["{'test_score': 0.9950728803120509,\n"," 'dev_score_history': [0.9960993635803737,\n","  0.9960993635803737,\n","  0.9895298706631082,\n","  0.9907616505850955,\n","  0.99096694723876,\n","  0.9897351673167727],\n"," 'train_loss_history': [0.016210260567105218,\n","  0.004237746983002985,\n","  0.005485479443716876,\n","  0.00465479899320337,\n","  0.005137028935071099,\n","  0.0073376248485700814],\n"," 'dev_loss_history': [0.004855041857808828,\n","  0.0031003127805888653,\n","  0.004632778000086546,\n","  0.005278266966342926,\n","  0.0043687643483281136,\n","  0.00709431292489171]}"]},"metadata":{},"execution_count":8}]}]}